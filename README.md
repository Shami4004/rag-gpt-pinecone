RAG Pipeline with GPT & Pinecone

This project demonstrates a fully functional Retrieval-Augmented Generation (RAG) pipeline using:
	â€¢	LangChain for orchestration
	â€¢	OpenAI GPT for natural language processing
	â€¢	Pinecone for vector storage and fast retrieval
	â€¢	Dotenv for secure environment variable handling

What It Does:
	â€¢	Splits a text document into manageable chunks.
	â€¢	Converts those chunks into embeddings using OpenAI.
	â€¢	Stores them in a Pinecone index.
	â€¢	Enables querying via GPT using the most relevant documents retrieved from Pinecone.

Project Structure:
rag-gpt-pinecone/
â”œâ”€â”€ .env.example         â€“> Environment variable template
â”œâ”€â”€ requirements.txt     â€“> Python dependencies
â”œâ”€â”€ setup_instructions.md â€“> Detailed setup guide
â”œâ”€â”€ your_file.txt        â€“> Sample input file
â””â”€â”€ src/
â””â”€â”€ rag_pipeline.py  â€“> Main pipeline script

Quick Start:
	1.	Clone the Repository:
git clone https://github.com/your-org/rag-gpt-pinecone.git
cd rag-gpt-pinecone
	2.	Create a Virtual Environment:
python -m venv env
source env/bin/activate  (or env\Scripts\activate on Windows)
	3.	Install Dependencies:
pip install -r requirements.txt
	4.	Configure Environment:
Rename .env.example to .env and add:
OPENAI_API_KEY=your_openai_key
PINECONE_API_KEY=your_pinecone_key
	5.	Add Your Text:
Edit the raw_text section in rag_pipeline.py or load your file.
	6.	Run the Pipeline:
python src/rag_pipeline.py

Expected Output Example:
âœ… Successfully added documents to Pinecone using new API.
ðŸ“„ Query Result:
The sample text is about LangChain enabling retrieval-augmented generation (RAG) with Pinecone, which is useful for LLM-powered apps.

Author:
Muhammad Ahtesham Ahmad
Engineer â€¢ Entrepreneur â€¢ AI Expert
Email: iamshami1996@gmail.com